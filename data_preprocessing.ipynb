{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11f761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd; import numpy as np;\n",
    "import os; import math; import copy\n",
    "#import torch; import torch.nn as nn\n",
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d13b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(text, num_scripts=4, batch_size=64, block_size=256):\n",
    "    data = list(text.sample(num_scripts)['content'])\n",
    "    data = tokenizer(data, truncation=True)['input_ids']\n",
    "    data = [add_section_tokens(sent) for sent in data if len(sent)>block_size]\n",
    "    X, Y = format_one_script(data[0], batch_size=batch_size, block_size=block_size)\n",
    "    for d in data[1:]:\n",
    "        x, y = format_one_script(d, batch_size=batch_size, block_size=block_size)\n",
    "        X, Y = torch.concat([X, x], dim=0), torch.concat([Y, y], dim=0)\n",
    "    del data\n",
    "    return X.to(device), Y.to(device)\n",
    "\n",
    "def add_section_tokens(sent, prob=0.5):\n",
    "    sent_break_length = len(sent)//3\n",
    "    pre = pre_s + sent[:sent_break_length] + pre_e\n",
    "    mid = sent[sent_break_length:len(sent)-sent_break_length]\n",
    "    mid = mid_s + mid + mid_e\n",
    "    suf = suf_s + sent[-sent_break_length:] + suf_e\n",
    "    return pre + mid + suf if np.random.random(1)[0] >= prob else pre + suf + mid\n",
    "\n",
    "def format_one_script(data, batch_size=64, block_size=256):\n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    del data\n",
    "    return x, y\n",
    "\n",
    "#data\n",
    "#path = '/Users/kchen/Documents/Machine Learning/Models/Transformer Architecture/CodeLLM/Code/data/the_stack/'\n",
    "#text = pd.read_parquet(path+'data-00000-of-00144.parquet')\n",
    "#text = text[text['size']>1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17188ead",
   "metadata": {},
   "source": [
    "token indices sequence length is longer than the specified maximum sequence length\n",
    "\n",
    "This means you're encoding a sequence that is larger than the max sequence the model can handle (which is 512 tokens). This is not an error but a warning; if you pass that sequence to the model it will crash as it cannot handle such a long sequence.\n",
    "\n",
    "You can truncate the sequence: seq = seq[:512] or use the max_length tokenizer parameter so that it handles it on its own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
