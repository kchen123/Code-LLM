{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59b9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math; import copy\n",
    "#torch\n",
    "import torch; import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578e2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: Karpathy's Shakespeare GPT\n",
    "class CodeLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head)\n",
    "                                      for _ in range(n_layer)])\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "        self.generator = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "    #apply weights: important according to Karpathy    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    #to train model        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        #Embeddings\n",
    "        tok_emb = self.token_embedding(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C) = token_embed + positional_embedding\n",
    "        #Apply Decoder Layers + LayerNorm\n",
    "        x = self.norm(self.blocks(x))\n",
    "        #Final Output\n",
    "        logits = self.generator(x) # (B,T,tgt_vocab)\n",
    "        #Calculate Loss: pred word v.s. next word\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    #to generate code given pref-code\n",
    "    def generate(self, idx, max_new_tokens=200):\n",
    "        #B, T = idx.shape\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            if idx_next.cpu().tolist()[0][0] == PAD[0]:\n",
    "                break\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a8cfa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        head_size = n_embd // n_head\n",
    "        #choose either MHA or MQA\n",
    "        #self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.sa = MultiQueryAttention(n_head, head_size)\n",
    "        self.ff = PositionwiseFeedForward(n_embd)\n",
    "        self.norm1 = nn.LayerNorm(n_embd)\n",
    "        self.norm2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        #normalize by feature, self-attention + residual\n",
    "        x = x + self.sa(self.norm1(x))\n",
    "        #normalize by feature, feed-forward + residual\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "    \n",
    "class OneHeadAttention(nn.Module):\n",
    "    def __init__(self, head_size, n_embd, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #wq, wk and wv\n",
    "        self.linears = clones(nn.Linear(n_embd, head_size, bias=False), 3)\n",
    "        #mask -> to discuss\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    def forward(self, x):\n",
    "        #T can differ during eval\n",
    "        B, T, C = x.shape\n",
    "        query, key, value = [\n",
    "            l(x) for l in self.linears\n",
    "        ]\n",
    "        d_k = query.size(-1)\n",
    "        #apply the attn equation\n",
    "        wei = (query @ key.transpose(-2, -1))/math.sqrt(d_k)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = self.dropout(F.softmax(wei, dim=-1))\n",
    "        wei = wei @ value\n",
    "        #delete variables to save space\n",
    "        del query, key, value\n",
    "        return wei \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([OneHeadAttention(head_size, n_embd) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size,  dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_embed = num_heads * head_size\n",
    "        self.num_heads, self.head_size = num_heads, head_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #wq (multiple heads), wk (1 head), wv(1 head), and wo\n",
    "        self.wq = nn.Linear(self.n_embed, self.n_embed).to(device)\n",
    "        self.wk = nn.Linear(self.n_embed, self.head_size).to(device)\n",
    "        self.wv = nn.Linear(self.n_embed, self.head_size).to(device)\n",
    "        self.wo = nn.Linear(self.n_embed, self.n_embed).to(device)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_size])).to(device)\n",
    "        self.tril = torch.concat([torch.tril(torch.ones(batch_size, block_size))\n",
    "                                  for _ in range(num_scripts)], dim=0).to(device)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        #x = [batch size, xlen, hid dim]\n",
    "        #query, key, value\n",
    "        query = self.wq(x).view(B, -1, self.num_heads, self.head_size).permute(0, 2, 1, 3)\n",
    "        key = self.wk(x).view(B, -1, 1, self.head_size).permute(0, 2, 3, 1)\n",
    "        value = self.wv(x).view(B, -1, 1, self.head_size).permute(0, 2, 1, 3)\n",
    "        #apply attention equation\n",
    "        scores = (query @ key)/self.scale #[batch size, n heads, query len, key len]\n",
    "        scores = scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        p_attn = F.softmax(scores, dim = -1) #[batch size, n heads, query len, key len]\n",
    "        x = self.dropout(p_attn) @ value\n",
    "        #reshape\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(B, -1, self.n_embed)\n",
    "        #delete variables to save space\n",
    "        del query, key, value, scores, p_attn\n",
    "        return self.wo(x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(n_embd, n_embd * 4)\n",
    "        self.w2 = nn.Linear(n_embd * 4, n_embd)\n",
    "        self.dropout =  nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.w1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.w2(x)\n",
    "\n",
    "#helper\n",
    "def clones(layer, N):\n",
    "    return nn.ModuleList([\n",
    "        copy.deepcopy(layer)\n",
    "        for _ in range(N)\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
